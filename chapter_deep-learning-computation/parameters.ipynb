{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa1c1bb",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 参数管理\n",
    "\n",
    "在选择了架构并设置了超参数后，我们就进入了训练阶段。\n",
    "此时，我们的目标是找到使损失函数最小化的模型参数值。\n",
    "经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "此外，有时我们希望提取参数，以便在其他环境中复用它们，\n",
    "将模型保存下来，以便它可以在其他软件中执行，\n",
    "或者为了获得科学的理解而进行检查。\n",
    "\n",
    "之前的介绍中，我们只依靠深度学习框架来完成训练的工作，\n",
    "而忽略了操作参数的具体细节。\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化；\n",
    "* 参数初始化；\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先看一下具有单隐藏层的多层感知机。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fb80c1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:45.734090Z",
     "iopub.status.busy": "2023-08-14T19:15:45.733731Z",
     "iopub.status.idle": "2023-08-14T19:15:49.273808Z",
     "shell.execute_reply": "2023-08-14T19:15:49.272911Z"
    },
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[0.48614687],\n",
       "       [0.28035107]], dtype=float32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "X = tf.random.uniform((2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f1a96b",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## [**参数访问**]\n",
    "\n",
    "我们从已有模型中访问参数。\n",
    "当通过`Sequential`类定义模型时，\n",
    "我们可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "如下所示，我们可以检查第二个全连接层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abbb56b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.277508Z",
     "iopub.status.busy": "2023-08-14T19:15:49.277212Z",
     "iopub.status.idle": "2023-08-14T19:15:49.284062Z",
     "shell.execute_reply": "2023-08-14T19:15:49.282936Z"
    },
    "origin_pos": 8,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_1/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[ 0.22134173],\n",
      "       [-0.04413009],\n",
      "       [ 0.28584135],\n",
      "       [ 0.8104315 ]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(net.layers[2].weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75940179",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "输出的结果告诉我们一些重要的事情：\n",
    "首先，这个全连接层包含两个参数，分别是该层的权重和偏置。\n",
    "两者都存储为单精度浮点数（float32）。\n",
    "注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。\n",
    "\n",
    "### [**目标参数**]\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。\n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909eee15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.287437Z",
     "iopub.status.busy": "2023-08-14T19:15:49.287148Z",
     "iopub.status.idle": "2023-08-14T19:15:49.293120Z",
     "shell.execute_reply": "2023-08-14T19:15:49.292275Z"
    },
    "origin_pos": 12,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "<tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(type(net.layers[2].weights[1]))\n",
    "print(net.layers[2].weights[1])\n",
    "print(tf.convert_to_tensor(net.layers[2].weights[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612bd47",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "### [**一次性访问所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要递归整个树来提取每个子块的参数。\n",
    "下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f0e280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.296522Z",
     "iopub.status.busy": "2023-08-14T19:15:49.296216Z",
     "iopub.status.idle": "2023-08-14T19:15:49.303901Z",
     "shell.execute_reply": "2023-08-14T19:15:49.302962Z"
    },
    "origin_pos": 20,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[ 0.5751061 , -0.4152906 ,  0.01401705,  0.60802716],\n",
      "       [ 0.6330345 ,  0.27219015,  0.6282534 ,  0.42685097],\n",
      "       [-0.6187873 ,  0.36100715, -0.11287123, -0.688246  ],\n",
      "       [ 0.07611448, -0.5624621 , -0.41585347, -0.33154744]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "[array([[ 0.5751061 , -0.4152906 ,  0.01401705,  0.60802716],\n",
      "       [ 0.6330345 ,  0.27219015,  0.6282534 ,  0.42685097],\n",
      "       [-0.6187873 ,  0.36100715, -0.11287123, -0.688246  ],\n",
      "       [ 0.07611448, -0.5624621 , -0.41585347, -0.33154744]],\n",
      "      dtype=float32), array([0., 0., 0., 0.], dtype=float32), array([[ 0.22134173],\n",
      "       [-0.04413009],\n",
      "       [ 0.28584135],\n",
      "       [ 0.8104315 ]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(net.layers[1].weights)\n",
    "print(net.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b146c",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de7cdc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.307261Z",
     "iopub.status.busy": "2023-08-14T19:15:49.306969Z",
     "iopub.status.idle": "2023-08-14T19:15:49.313592Z",
     "shell.execute_reply": "2023-08-14T19:15:49.312724Z"
    },
    "origin_pos": 24,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_weights()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f0f4f",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "### [**从嵌套块收集参数**]\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。\n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28084d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.316849Z",
     "iopub.status.busy": "2023-08-14T19:15:49.316559Z",
     "iopub.status.idle": "2023-08-14T19:15:49.446411Z",
     "shell.execute_reply": "2023-08-14T19:15:49.445553Z"
    },
    "origin_pos": 29,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[0.],\n",
       "       [0.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1(name):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4, activation=tf.nn.relu)],\n",
    "        name=name)\n",
    "\n",
    "def block2():\n",
    "    net = tf.keras.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add(block1(name=f'block-{i}'))\n",
    "    return net\n",
    "\n",
    "rgnet = tf.keras.Sequential()\n",
    "rgnet.add(block2())\n",
    "rgnet.add(tf.keras.layers.Dense(1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0463d05",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "[**设计了网络后，我们看看它是如何工作的。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ddb9129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.450206Z",
     "iopub.status.busy": "2023-08-14T19:15:49.449596Z",
     "iopub.status.idle": "2023-08-14T19:15:49.464188Z",
     "shell.execute_reply": "2023-08-14T19:15:49.463271Z"
    },
    "origin_pos": 34,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Layer (type)                Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sequential_2 (Sequential)   (2, 4)                    80        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_6 (Dense)             (2, 1)                    5         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-trainable params: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(rgnet.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e780a85",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。\n",
    "下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d38ed0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.467908Z",
     "iopub.status.busy": "2023-08-14T19:15:49.467276Z",
     "iopub.status.idle": "2023-08-14T19:15:49.473768Z",
     "shell.execute_reply": "2023-08-14T19:15:49.472936Z"
    },
    "origin_pos": 38,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet.layers[0].layers[1].layers[1].weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a68b5",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "## 参数初始化\n",
    "\n",
    "知道了如何访问参数后，现在我们看看如何正确地初始化参数。\n",
    "我们在 :numref:`sec_numerical_stability`中讨论了良好初始化的必要性。\n",
    "深度学习框架提供默认随机初始化，\n",
    "也允许我们创建自定义初始化方法，\n",
    "满足我们通过其他规则实现初始化权重。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1276063",
   "metadata": {
    "origin_pos": 43,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "默认情况下，Keras会根据一个范围均匀地初始化权重矩阵，\n",
    "这个范围是根据输入和输出维度计算出的。\n",
    "偏置参数设置为0。\n",
    "TensorFlow在根模块和`keras.initializers`模块中提供了各种初始化方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e590f",
   "metadata": {
    "origin_pos": 45
   },
   "source": [
    "### [**内置初始化**]\n",
    "\n",
    "让我们首先调用内置的初始化器。\n",
    "下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，\n",
    "且将偏置参数设置为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5924fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.477519Z",
     "iopub.status.busy": "2023-08-14T19:15:49.476898Z",
     "iopub.status.idle": "2023-08-14T19:15:49.509508Z",
     "shell.execute_reply": "2023-08-14T19:15:49.508663Z"
    },
    "origin_pos": 48,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_7/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[ 0.00482195, -0.01083507, -0.01526479, -0.00534363],\n",
       "        [-0.00700699, -0.00109939,  0.01193621, -0.00653713],\n",
       "        [ 0.00322435,  0.01800844,  0.01075094, -0.00492984],\n",
       "        [ 0.01004681, -0.00403099, -0.0229763 ,  0.02212855]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_7/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "        bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c266f47",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "我们还可以将所有参数初始化为给定的常数，比如初始化为1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af26d8a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.512987Z",
     "iopub.status.busy": "2023-08-14T19:15:49.512692Z",
     "iopub.status.idle": "2023-08-14T19:15:49.545232Z",
     "shell.execute_reply": "2023-08-14T19:15:49.544390Z"
    },
    "origin_pos": 53,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_9/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'dense_9/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.keras.initializers.Constant(1),\n",
    "        bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92485fa7",
   "metadata": {
    "origin_pos": 55
   },
   "source": [
    "我们还可以[**对某些块应用不同的初始化方法**]。\n",
    "例如，下面我们使用Xavier初始化方法初始化第一个神经网络层，\n",
    "然后将第三个神经网络层初始化为常量值42。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f537f025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.548627Z",
     "iopub.status.busy": "2023-08-14T19:15:49.548330Z",
     "iopub.status.idle": "2023-08-14T19:15:49.577236Z",
     "shell.execute_reply": "2023-08-14T19:15:49.576355Z"
    },
    "origin_pos": 58,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_11/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[ 0.44712347, -0.79997337, -0.37350926,  0.7941168 ],\n",
      "       [-0.5101963 ,  0.5134434 ,  0.26661164, -0.15044606],\n",
      "       [ 0.03675801,  0.7210526 , -0.20288467,  0.29654735],\n",
      "       [ 0.29434448,  0.80070907, -0.5268253 ,  0.32967573]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'dense_12/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.keras.initializers.GlorotUniform()),\n",
    "    tf.keras.layers.Dense(\n",
    "        1, kernel_initializer=tf.keras.initializers.Constant(1)),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "print(net.layers[1].weights[0])\n",
    "print(net.layers[2].weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad6cd3",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "### [**自定义初始化**]\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd9b57a",
   "metadata": {
    "origin_pos": 63,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "在这里，我们定义了一个`Initializer`的子类，\n",
    "并实现了`__call__`函数。\n",
    "该函数返回给定形状和数据类型的所需张量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1510888f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.580878Z",
     "iopub.status.busy": "2023-08-14T19:15:49.580574Z",
     "iopub.status.idle": "2023-08-14T19:15:49.611304Z",
     "shell.execute_reply": "2023-08-14T19:15:49.610436Z"
    },
    "origin_pos": 67,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_13/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[ 5.5029793,  0.       , -5.9067774,  6.986786 ],\n",
      "       [ 6.385519 ,  5.0954227,  0.       ,  6.8196716],\n",
      "       [-0.       , -8.512001 ,  5.8291416, -0.       ],\n",
      "       [-0.       , -8.126636 , -0.       , -9.668117 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "class MyInit(tf.keras.initializers.Initializer):\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        data=tf.random.uniform(shape, -10, 10, dtype=dtype)\n",
    "        factor=(tf.abs(data) >= 5)\n",
    "        factor=tf.cast(factor, tf.float32)\n",
    "        return data * factor\n",
    "\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=MyInit()),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "print(net.layers[1].weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2d884",
   "metadata": {
    "origin_pos": 69
   },
   "source": [
    "注意，我们始终可以直接设置参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c37c407c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.614805Z",
     "iopub.status.busy": "2023-08-14T19:15:49.614506Z",
     "iopub.status.idle": "2023-08-14T19:15:49.623355Z",
     "shell.execute_reply": "2023-08-14T19:15:49.622512Z"
    },
    "origin_pos": 72,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_13/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       "array([[42.       ,  1.       , -4.9067774,  7.986786 ],\n",
       "       [ 7.385519 ,  6.0954227,  1.       ,  7.8196716],\n",
       "       [ 1.       , -7.512001 ,  6.8291416,  1.       ],\n",
       "       [ 1.       , -7.1266356,  1.       , -8.668117 ]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[1].weights[0][:].assign(net.layers[1].weights[0] + 1)\n",
    "net.layers[1].weights[0][0, 0].assign(42)\n",
    "net.layers[1].weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972e1b7",
   "metadata": {
    "origin_pos": 75
   },
   "source": [
    "## [**参数绑定**]\n",
    "\n",
    "有时我们希望在多个层间共享参数：\n",
    "我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acf8d711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:15:49.626570Z",
     "iopub.status.busy": "2023-08-14T19:15:49.626278Z",
     "iopub.status.idle": "2023-08-14T19:15:49.654113Z",
     "shell.execute_reply": "2023-08-14T19:15:49.653212Z"
    },
    "origin_pos": 78,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# tf.keras的表现有点不同。它会自动删除重复层\n",
    "shared = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    shared,\n",
    "    shared,\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "# 检查参数是否不同\n",
    "print(len(net.layers) == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c448ee60",
   "metadata": {
    "origin_pos": 82
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 使用 :numref:`sec_model_construction` 中定义的`FancyMLP`模型，访问各个层的参数。\n",
    "1. 查看初始化模块文档以了解不同的初始化方法。\n",
    "1. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "1. 为什么共享参数是个好主意？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce139a",
   "metadata": {
    "origin_pos": 85,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1830)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}